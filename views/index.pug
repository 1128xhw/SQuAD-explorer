extends layout

block title
  title The Stanford Question Answering Dataset

block description
  meta(name='description', content='Stanford Question Answering Dataset (SQuAD) is a new reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage. With 100,000+ question-answer pairs on 500+ articles, SQuAD is significantly larger than previous reading comprehension datasets.')

block extralinks
  link(rel='stylesheet', href='/stylesheets/index.css')
  link(href='https://cdn-images.mailchimp.com/embedcode/horizontal-slim-10_7.css', rel='stylesheet', type='text/css')
  script(async defer src="https://buttons.github.io/buttons.js")

block extrascripts

block content
  .cover#contentCover
    .container
      .row
        .col-md-8.col-md-offset-2
          .infoCard
            .infoBody
              .infoHeadline
                h2 What is SQuAD?
              p 
                span
                  b S
                  |tanford 
                  b Qu
                  |estion 
                  b A
                  |nswering 
                  b D
                  |ataset (SQuAD) 
                | is a new reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or 
                i span
                | , from the corresponding reading passage. With 100,000+ question-answer pairs on 500+ articles, SQuAD is significantly larger than previous reading comprehension datasets.
              .infoHeadline
                h2 Leaderboard
              p 
                | Since the release of our dataset (
                a(href="http://arxiv.org/abs/1606.05250") and paper
                | ), the community has made rapid progress! Here are the ExactMatch (EM) and F1 scores of the best models evaluated on the test and development sets of v1.1. Will your model outperform humans on the QA task?
              table.table#performanceTable
                tr
                  th Model
                  th Test EM
                  th Test F1
                  th Dev EM
                  th Dev F1
                tr
                  td
                    |  Human Performance <br> (Stanford University) <br>
                    a(href="http://arxiv.org/abs/1606.05250") (Rajpurkar et al. '16)
                  td 82.3
                  td 91.2
                  td 81.4
                  td 91.0
                tr
                  td
                    | r-net <br> (Microsoft Research Asia)
                  td TBD
                  td TBD
                  td
                    b 65.8
                  td
                    b 75.0
                tr
                  td 
                    | Co-attention <br> (Allen Institute for Artificial Intelligence &amp; University of Washington)
                  td 
                    b 61.8
                  td
                    b 72.5
                  td 62.2
                  td 72.6
                tr
                  td 
                    | No Chunker and Neural Chunk Ranker with Fine Attention <br> (IBM)
                  td TBD
                  td TBD
                  td 61.8
                  td 70.7
                tr
                  td
                    | Match-LSTM with Ans-Ptr (Boundary) <br> (Singapore Management University) <br>
                    a(href="http://arxiv.org/abs/1608.07905") (Wang &amp; Jiang '16)
                  td 60.5
                  td 70.7
                  td 59.4
                  td 70.0
                tr
                  td
                    | Trained Chunker and Neural Chunk Ranker <br> (IBM)
                  td TBD
                  td TBD
                  td 58.7
                  td 68.4
                tr
                  td
                    | Match-LSTM with Ans-Ptr (Sequence) <br> (Singapore Management University) <br>
                    a(href="http://arxiv.org/abs/1608.07905") (Wang &amp; Jiang '16)
                  td 54.5
                  td 67.7
                  td 54.8
                  td 68.0
                tr
                  td
                    | Attention and Chunking Single Model<br> (IBM)
                  td TBD
                  td TBD
                  td 48.0
                  td 64.5
                tr
                  td
                    |  Logistic Regression Baseline <br> (Stanford University) <br> 
                    a(href="http://arxiv.org/abs/1606.05250") (Rajpurkar et al. '16)
                  td 40.4
                  td 51.0
                  td 39.8
                  td 51.0
              .infoHeadline
                h2 Getting Started
              p
                | We've built a few resources to help you get started with the dataset.
              p To get a feel of the dataset, you can explore it visually. 
              a.btn.actionBtn.inverseBtn(href="/explore/1.1/dev/") Explore SQuAD
              p 
                | Download a copy of the dataset (distributed under the  
                a(href="http://creativecommons.org/licenses/by-sa/4.0/legalcode") CC BY-SA 4.0
                |  license):
                ul.list-unstyled
                  li
                    a.btn.actionBtn.inverseBtn(href="/dataset/train-v1.1.json", download)
                      | Training Set v1.1 (30 MB)
                  li
                    a.btn.actionBtn.inverseBtn(href="/dataset/dev-v1.1.json", download)
                      | Dev Set v1.1 (5 MB)
              p To evaluate your models, we have also made available the evaluation script we will use for official evaluation, along with a sample prediction file that the script will take as input. To run the evaluation, use 
                code
                  | python evaluate-v1.1.py &lt;path_to_dev-v1.1&gt; &lt;path_to_predictions&gt;
                |.
                ul.list-unstyled
                  li
                    a.btn.actionBtn.inverseBtn(href="https://worksheets.codalab.org/rest/bundles/0xbcd57bee090b421c982906709c8c27e1/contents/blob/", download)
                      | Evaluation Script v1.1
                  li
                    a.btn.actionBtn.inverseBtn(href="https://worksheets.codalab.org/rest/bundles/0xc83bf36cf8714819ba11802b59cb809e/contents/blob/", download)
                      | Sample Prediction File (on Dev v1.1)
              p Once you have a built a model that works to your expectations on the dev set, you submit it to get official scores on the dev and a hidden test set. To preserve the integrity of test results, we do not release the test set to the public. Instead, we require you to submit your model so that we can run it on the test set for you. Here's a tutorial walking you through official evaluation of your model:
              a.btn.actionBtn.inverseBtn(href="https://worksheets.codalab.org/worksheets/0x8403d867f9a3444685c344f4f0bc8d34/")
                | Submission Tutorial
              p Because SQuAD is an ongoing effort, we expect the dataset to evolve.
              p To keep up to date with major changes to the dataset, please subscribe:
              include includes/mailchimp
              .infoHeadline
                h2 Have Questions?
              p 
                | Ask us questions at our   
                a(href="https://groups.google.com/forum/#!forum/squad-stanford-qa") google group
                |  or at 
                a(href="mailto:pranavsr@stanford.edu") pranavsr@stanford.edu
                | .
            .infoSubheadline
              include includes/tweet
              include includes/github

<!DOCTYPE html><!--Author: Pranav Rajpurkar 2016--><html><head><meta charset="utf-8"><title>The Stanford Question Answering Dataset</title><meta name="description" content="Stanford Question Answering Dataset (SQuAD) is a new reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage. With 100,000+ question-answer pairs on 500+ articles, SQuAD is significantly larger than previous reading comprehension datasets."><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no"><meta property="og:image" content="/logo.jpg"><link rel="image_src" type="image/jpeg" href="/SQuAD-explorer/logo.jpg"><link rel="shortcut icon" href="/SQuAD-explorer/favicon.ico" type="image/x-icon"><link rel="icon" href="/SQuAD-explorer/favicon.ico" type="image/x-icon"><link rel="stylesheet" href="/SQuAD-explorer/bower_components/bootstrap/dist/css/bootstrap.min.css"><link rel="stylesheet" href="/SQuAD-explorer/stylesheets/layout.css"><link rel="stylesheet" href="/SQuAD-explorer/stylesheets/index.css"><link href="https://cdn-images.mailchimp.com/embedcode/horizontal-slim-10_7.css" rel="stylesheet" type="text/css"><script async defer src="https://buttons.github.io/buttons.js"></script><script src="/SQuAD-explorer/javascripts/analytics.js"></script></head><body><div class="navbar navbar-default navbar-fixed-top" id="topNavbar" role="navigation"><div class="container clearfix" id="navContainer"><div class="rightNav"><div class="collapseDiv"><button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar"><span class="glyphicon glyphicon-menu-hamburger"></span></button></div><div class="collapse navbar-collapse" id="navbar"><ul class="nav navbar-nav navbar-right"><li><a href="/SQuAD-explorer/">Home</a></li><li><a href="/SQuAD-explorer/explore/1.1/dev/">Explore</a></li></ul></div></div><div class="leftNav"><div class="brandDiv"><a class="navbar-brand" href="/SQuAD-explorer/">SQuAD</a></div></div></div></div><div class="cover" id="topCover"><div class="container"><div class="row"><div class="col-md-12"><h1 id="appTitle">SQuAD</h1><h2 id="appSubtitle">The Stanford Question Answering Dataset</h2></div></div></div></div><div class="cover" id="contentCover"><div class="container"><div class="row"><div class="col-md-5"><div class="infoCard"><div class="infoBody"><div class="infoHeadline"><h2>What is SQuAD?</h2></div><p> <span><b>S</b>tanford <b>Qu</b>estion <b>A</b>nswering <b>D</b>ataset (SQuAD) </span>is a new reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or <i>span</i>, from the corresponding reading passage. With 100,000+ question-answer pairs on 500+ articles, SQuAD is significantly larger than previous reading comprehension datasets.</p><a class="btn actionBtn" href="/SQuAD-explorer/explore/1.1/dev/">Explore SQuAD and model predictions</a><a class="btn actionBtn" href="http://arxiv.org/abs/1606.05250">Read the paper (Rajpurkar et. al, 2016)</a><div class="infoHeadline"><h2>Getting Started</h2></div><p>We've built a few resources to help you get started with the dataset.</p><p> Download a copy of the dataset (distributed under the  <a href="http://creativecommons.org/licenses/by-sa/4.0/legalcode">CC BY-SA 4.0</a> license):<ul class="list-unstyled"><li><a class="btn actionBtn inverseBtn" href="/SQuAD-explorer/dataset/train-v1.1.json" download>Training Set v1.1 (30 MB)</a></li><li><a class="btn actionBtn inverseBtn" href="/SQuAD-explorer/dataset/dev-v1.1.json" download>Dev Set v1.1 (5 MB)</a></li></ul></p><p>To evaluate your models, we have also made available the evaluation script we will use for official evaluation, along with a sample prediction file that the script will take as input. To run the evaluation, use <code>python evaluate-v1.1.py &lt;path_to_dev-v1.1&gt; &lt;path_to_predictions&gt;</code>.<ul class="list-unstyled"><li><a class="btn actionBtn inverseBtn" href="https://worksheets.codalab.org/rest/bundles/0xbcd57bee090b421c982906709c8c27e1/contents/blob/" download>Evaluation Script v1.1</a></li><li><a class="btn actionBtn inverseBtn" href="https://worksheets.codalab.org/rest/bundles/0xc83bf36cf8714819ba11802b59cb809e/contents/blob/" download>Sample Prediction File (on Dev v1.1)</a></li></ul></p><p>Once you have a built a model that works to your expectations on the dev set, you submit it to get official scores on the dev and a hidden test set. To preserve the integrity of test results, we do not release the test set to the public. Instead, we require you to submit your model so that we can run it on the test set for you. Here's a tutorial walking you through official evaluation of your model:</p><a class="btn actionBtn inverseBtn" href="https://worksheets.codalab.org/worksheets/0x8403d867f9a3444685c344f4f0bc8d34/">Submission Tutorial</a><p>Because SQuAD is an ongoing effort, we expect the dataset to evolve.</p><p>To keep up to date with major changes to the dataset, please subscribe:</p><div id="mc_embed_signup"><form class="validate" id="mc-embedded-subscribe-form" action="//google.us13.list-manage.com/subscribe/post?u=1842e6560d6e10316b4e1aaf5&amp;id=76586bdcf4" method="post" name="mc-embedded-subscribe-form" target="_blank" novalidate=""><div id="mc_embed_signup_scroll"><input class="email" id="mce-EMAIL" type="email" value="" name="EMAIL" placeholder="email address" required=""><div style="position: absolute; left: -5000px;" aria-hidden="true"><input type="text" name="b_1842e6560d6e10316b4e1aaf5_76586bdcf4" tabindex="-1" value=""></div><div class="clear"><input class="button" id="mc-embedded-subscribe" type="submit" value="Subscribe" name="subscribe"></div></div></form></div><div class="infoHeadline"><h2>Have Questions?</h2></div><p> Ask us questions at our   <a href="https://groups.google.com/forum/#!forum/squad-stanford-qa">google group</a> or at <a href="mailto:pranavsr@stanford.edu">pranavsr@stanford.edu</a>.</p></div><div class="infoSubheadline"><a href="https://twitter.com/share" class="twitter-share-button" data-url="https://stanford-qa.com" data-text="The Stanford Question Answering Dataset - 100,000+ questions for reading comprehension" data-via="stanfordnlp" data-size="large" data-hashtags="SQuAD">Tweet</a> <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script><!-- Place this tag where you want the button to render. -->
<a class="github-button" href="https://github.com/rajpurkar/SQuAD-explorer" data-icon="octicon-star" data-style="mega" data-count-href="/rajpurkar/SQuAD-explorer/stargazers" data-count-api="/repos/rajpurkar/SQuAD-explorer#stargazers_count" data-count-aria-label="# stargazers on GitHub" aria-label="Star rajpurkar/SQuAD-explorer on GitHub">Star</a></div></div></div><div class="col-md-7"><div class="infoCard"><div class="infoBody"><div class="infoHeadline"><h2>Test Set Leaderboard</h2></div><p> Since the release of our dataset, the community has made rapid progress! Here are the ExactMatch (EM) and F1 scores of the best models evaluated on the test and development sets of v1.1. Will your model outperform humans on the QA task?</p><table class="table performanceTable"><tr><th>Rank</th><th>Model</th><th>EM</th><th>F1</th></tr><tr><td>1</td><td> r-net (ensemble)<p class="institution">Microsoft Research Asia</p></td><td>75.863</td><td>82.947</td></tr><tr><td>2</td><td> ReasoNet (ensemble)<p class="institution">MSR Redmond</p></td><td>73.419</td><td>81.752</td></tr><tr><td>2</td><td> Multi-Perspective Matching (diversity-ensemble)<p class="institution">IBM Research</p></td><td>73.765</td><td>81.257</td></tr><tr><td>3</td><td> BiDAF (ensemble)<p class="institution">Allen Institute for AI &amp; University of Washington</p></td><td>73.314</td><td>81.089</td></tr><tr><td>4</td><td> Dynamic Coattention Networks (ensemble) <p class="institution">Salesforce Research</p></td><td>71.625</td><td>80.383</td></tr><tr><td>5</td><td> r-net (single model)<p class="institution">Microsoft Research Asia</p></td><td>71.258</td><td>79.66</td></tr><tr><td>6</td><td> Document Reader (single model)<p class="institution">Facebook AI Research</p></td><td>69.967</td><td>78.974</td></tr><tr><td>7</td><td> ReasoNet (single model)<p class="institution">MSR Redmond</p></td><td>69.107</td><td>78.895</td></tr><tr><td>7</td><td> FastQAExt<p class="institution">German Research Center for Artificial Intelligence</p></td><td>70.849</td><td>78.857</td></tr><tr><td>8</td><td> Multi-Perspective Matching (single model)<p class="institution">IBM Research</p></td><td>68.877</td><td>77.771</td></tr><tr><td>9</td><td> jNet (single model)<p class="institution">USTC &amp; National Research Council Canada &amp; York University</p></td><td>68.73</td><td>77.393</td></tr><tr><td>10</td><td> BiDAF (single model)<p class="institution">Allen Institute for AI &amp; University of Washington</p></td><td>67.974</td><td>77.323</td></tr><tr><td>10</td><td> FastQA<p class="institution">German Research Center for Artificial Intelligence</p></td><td>68.436</td><td>77.07</td></tr><tr><td>11</td><td> Match-LSTM with Ans-Ptr (Boundary+Ensemble)<p class="institution">Singapore Management University</p></td><td>67.901</td><td>77.022</td></tr><tr><td>12</td><td> Iterative Co-attention Network<p class="institution">Fudan University</p></td><td>67.502</td><td>76.786</td></tr><tr><td>13</td><td> Dynamic Coattention Networks (single model) <p class="institution">Salesforce Research</p></td><td>66.233</td><td>75.896</td></tr><tr><td>13</td><td> RaSoR<p class="institution">Google NY, Tel-Aviv University</p></td><td>67.387</td><td>75.543</td></tr><tr><td>14</td><td> Match-LSTM with Bi-Ans-Ptr (Boundary)<p class="institution">Singapore Management University</p></td><td>64.744</td><td>73.743</td></tr><tr><td>15</td><td> Fine-Grained Gating<p class="institution">Carnegie Mellon University</p></td><td>62.446</td><td>73.327</td></tr><tr><td>15</td><td> Dynamic Chunk Reader<p class="institution">IBM</p></td><td>62.499</td><td>70.956</td></tr><tr><td>16</td><td> Match-LSTM with Ans-Ptr (Boundary)<p class="institution">Singapore Management University</p></td><td>60.474</td><td>70.695</td></tr><tr><td>17</td><td> Match-LSTM with Ans-Ptr (Sentence)<p class="institution">Singapore Management University</p></td><td>54.505</td><td>67.748</td></tr><tr class="human-row"><td></td><td>Human Performance<p class="institution">Stanford University</p><a href="http://arxiv.org/abs/1606.05250">(Rajpurkar et al. '16)</a></td><td>82.304</td><td>91.221</td></tr></table><div class="infoHeadline"><h2>Development Set Leaderboard</h2></div><p>While you are iterating on models, use the development set to get an indicator of your model's performance.</p><table class="table performanceTable"><tr><th>Model</th><th>EM</th><th>F1</th></tr><tr><td> RaSoR (ensemble)<p class="institution">Google NY</p></td><td>68.184</td><td>76.735</td></tr><tr><td> RaSoR<p class="institution">Google NY</p></td><td>66.433</td><td>74.941</td></tr><tr><td> Dynamic Chunk Ranker with Convolution layer<p class="institution">IBM</p></td><td>66.329</td><td>74.706</td></tr><tr><td> Attentive CNN context with LSTM<p class="institution">NLPR, CASIA</p></td><td>64.106</td><td>74.311</td></tr><tr><td> Stacked Bidirectional Neural network with Answer Pointer<p class="institution">Shri Mata Vaishno Devi University</p></td><td>62.763</td><td>72.902</td></tr><tr><td> Attentive Chunker<p class="institution">IBM</p></td><td>48.023</td><td>64.526</td></tr></table></div></div></div></div></div></div><nav class="navbar navbar-default navbar-static-bottom footer"><div class="container clearfix"><div class="rightNav"><div><ul class="nav navbar-nav navbar-right"><li><a href="/SQuAD-explorer/">SQuAD</a></li><li><a href="http://nlp.stanford.edu">Stanford NLP Group</a></li></ul></div></div></div></nav><script src="/SQuAD-explorer/bower_components/jquery/dist/jquery.min.js"></script><script src="/SQuAD-explorer/bower_components/bootstrap/dist/js/bootstrap.min.js"></script></body></html>
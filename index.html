<!DOCTYPE html><!--Author: Pranav Rajpurkar 2016--><html><head><meta charset="utf-8"><title>The Stanford Question Answering Dataset</title><meta name="description" content="Stanford Question Answering Dataset (SQuAD) is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or span, from the corresponding reading passage. With 107,785 question-answer pairs on 536 articles, SQuAD is significantly larger than previous reading comprehension datasets."><meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1, user-scalable=no"><meta property="og:image" content="/logo.jpg"><link rel="image_src" type="image/jpeg" href="/SQuAD-explorer/logo.jpg"><link rel="shortcut icon" href="/SQuAD-explorer/favicon.ico" type="image/x-icon"><link rel="icon" href="/SQuAD-explorer/favicon.ico" type="image/x-icon"><link rel="stylesheet" href="/SQuAD-explorer/bower_components/bootstrap/dist/css/bootstrap.min.css"><link rel="stylesheet" href="/SQuAD-explorer/stylesheets/layout.css"><link rel="stylesheet" href="/SQuAD-explorer/stylesheets/index.css"></head><body><div class="navbar navbar-default navbar-fixed-top" id="topNavbar" role="navigation"><div class="container clearfix" id="navContainer"><div class="rightNav"><div class="collapseDiv"><button class="navbar-toggle collapsed" type="button" data-toggle="collapse" data-target="#navbar" aria-expanded="false" aria-controls="navbar"><span class="glyphicon glyphicon-menu-hamburger"></span></button></div><div class="collapse navbar-collapse" id="navbar"><ul class="nav navbar-nav navbar-right"><li><a href="/SQuAD-explorer/">Home</a></li><li><a href="/SQuAD-explorer/explore.html">Explore</a></li></ul></div></div><div class="leftNav"><div class="brandDiv"><a class="navbar-brand" href="/SQuAD-explorer/">SQuAD</a></div></div></div></div><div class="cover" id="topCover"><div class="container"><div class="row"><div class="col-md-12"><h1 id="appTitle">SQuAD</h1><h2 id="appSubtitle">The Stanford Question Answering Dataset</h2></div></div></div></div><div class="cover" id="contentCover"><div class="container"><div class="row"><div class="col-md-8 col-md-offset-2"><div class="infoCard"><div class="infoBody"><div class="infoHeadline"><h2>What is SQuAD?</h2></div><p> <span style="white-space: nowrap"><b>S</b>tanford <b>Qu</b>estion <b>A</b>nswering <b>D</b>ataset (SQuAD) </span>is a reading comprehension dataset, consisting of questions posed by crowdworkers on a set of Wikipedia articles, where the answer to every question is a segment of text, or <i>span</i>, from the corresponding reading passage. With 107,785 question-answer pairs on 536 articles, SQuAD is significantly larger than previous reading comprehension datasets.</p><ul class="list-inline"><li><a class="btn actionBtn inverseBtn" href="http://arxiv.org/abs/1606.05250">Read Our Paper</a></li></ul><div class="spacer"></div><div class="infoHeadline"><h2>What does SQuAD look like?</h2></div><p>You can also browse through the full development and training sets.</p><ul class="list-inline"><li><a class="btn actionBtn inverseBtn" href="/SQuAD-explorer/explore.html">Explore Dev / Train Sets</a></li></ul><div class="spacer"></div><div class="infoHeadline"><h2>How do I get the dataset?</h2></div><p>You can download v1.0 of the training and development sets. </p><p>We also have a hidden test set. To evaluate your models on the test set, please get in contact with us.</p><p>Because SQuAD is an ongoing effort, we expect the dataset to evolve.</p><p>To keep up to date with major changes to the dataset, please subscribe:</p><div class="registerMessage"></div><form id="registerForm" action="/register" method="post" autocomplete="off"><div class="input-group"><input class="form-control" id="email" type="email" name="email" placeholder="Email Address" required="required"><div class="input-group-btn"><button class="btn" id="subscribeBtn" type="submit">Subscribe</button></div></div></form><p> The dataset is distributed under the  <a href="http://creativecommons.org/licenses/by-sa/4.0/legalcode">CC BY-SA 4.0</a> license.</p><div class="spacer"></div><div class="infoHeadline"><h2>What is the best model performance? </h2></div><p>Our best model (detailed in our  <a href="http://arxiv.org/abs/1606.05250">paper</a>) achieves an F1 score of <b>51.0%</b>.<br>We expect future models to close the gap to the human performance of <b>86.8%</b> .</p><ul class="list-inline"><li><a class="btn actionBtn inverseBtn" href="https://worksheets.codalab.org/worksheets/0x62eefc3e64e04430a1a24785a9293fff/">CodaLab Worksheet</a></li></ul><table class="table" id="performanceTable"><tr><th>Model</th><th>Dev F1</th><th>Test F1</th></tr><tr class="active"><td>Human Performance</td><td>90.5%</td><td>86.8%</td></tr><tr><td><a href="http://arxiv.org/abs/1606.05250">Rajpurkar et al. '16</a> Logistic Regression</td><td>51.0% </td><td>51.0%</td></tr><tr><td><b>Beat our model?</b> Contact us to get on the leaderboard.</td><td>?</td><td>?</td></tr></table><div class="spacer"></div><div class="infoHeadline"><h2>Have Questions?</h2></div><p> Contact us at <a href="mailto:pranavsr@stanford.edu">pranavsr@stanford.edu</a>.</p></div><div class="infoSubheadline"><a href="https://twitter.com/share" class="twitter-share-button" data-url="https://stanford-qa.com" data-text="The Stanford Question Answering Dataset - 100,000+ questions for reading comprehension" data-via="stanfordnlp" data-size="large" data-hashtags="SQuAD">Tweet</a> <script>!function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0],p=/^http:/.test(d.location)?'http':'https';if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src=p+'://platform.twitter.com/widgets.js';fjs.parentNode.insertBefore(js,fjs);}}(document, 'script', 'twitter-wjs');</script></div></div></div></div></div></div><nav class="navbar navbar-default navbar-static-bottom footer"><div class="container clearfix"><div class="rightNav"><div><ul class="nav navbar-nav navbar-right"><li><a href="/SQuAD-explorer/">SQuAD 2016</a></li><li><a href="http://nlp.stanford.edu">Stanford NLP Group</a></li></ul></div></div></div></nav><script src="/bower_components/jquery/dist/jquery.min.js"></script><script src="/bower_components/bootstrap/dist/js/bootstrap.min.js"></script></body></html>